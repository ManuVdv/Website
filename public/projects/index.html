<!DOCTYPE html>
<html lang="">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Manu Vanderveeren">
    <meta name="description" content="/">
    <meta name="keywords" content="blog,developer,personal">

    <meta property="og:site_name" content="HOME">
    <meta property="og:title" content="
  Projects - HOME
">
    <meta property="og:description" content="projects">
    <meta property="og:type" content="website">
    <meta property="og:url" content="/projects/">
    <meta property="og:image" content="/images/tn.png">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="/projects/">
    <meta name="twitter:image" content="/images/tn.png">

    <base href="/projects/">
    <title>
  Projects - HOME
</title>

    <link rel="canonical" href="/projects/">
    
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
    
    <link  rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700">
    <link rel="stylesheet" href="/css/normalize.min.css">
    <link rel="stylesheet" href="/css/style.min.css">

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    
      <link rel="alternate" href="/index.xml" type="application/rss+xml" title="HOME">
      <link href="/index.xml" rel="feed" type="application/rss+xml" title="HOME" />
    

    <meta name="generator" content="Hugo 0.75.1" />
  </head>

  <body class="">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">HOME</a>
    <input type="checkbox" id="menu-control"/>
    <label class="menu-mobile  float-right " for="menu-control">
      <span class="btn-mobile  float-right ">&#9776;</span>
      <ul class="navigation-list">
        
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="/about">About</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="/CV">CV</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="/projects">Projects</a>
            </li>
          
        
        
      </ul>
    </label>
  </section>
</nav>


      <div class="content">
        
  <section class="container page">
  <article>
    <header>
      <h1>Projects</h1>
    </header>

    <hr>
<h2 id="project-1-identifying-prostate-cancer">Project 1: Identifying prostate cancer</h2>
<p>Authors: Manu Vanderveeren &amp; Laura Bogaert</p>
<p><strong>1. Understanding the data</strong></p>
<p>This prostate dataset wants to predict a score called “Cscore” which describes the progression
of prostate cancer based on multiple variables such as size of the prostate, age of patient
among others.
After uploading the data, we checked the different dimensions, possible missing data, and
names of all columns.</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">&gt; dim(prostate)
[1] 97 8
&gt; sum(is.na(prostate))
[1] 0
&gt; names(prostate)
[1] &#34;Cscore&#34; &#34;lcavol&#34; &#34;lweight&#34; &#34;age&#34; &#34;lbph&#34; &#34;svi&#34; &#34;lcp&#34; &#34;lpsa&#34;
</code></pre></div><p>We found that our data has 97 data poins and 8 columns called “Cscore”, “lcavol”, “lweight”, “
age”, “lbph, &ldquo;svi&rdquo;, &ldquo;lcp&rdquo;, &ldquo;lpsa&rdquo; with no missing data.
To better understand the several variables, We plotted them individually to Cscore.</p>
<p><img src="/images/Cscore.jpg" alt="Cscore"></p>
<p>Evaluating the graphs, we find three items that require further investigation.
Firstly, we see that svi’s x-values are all 0 or 1. This suggests the variable is categorial. Secondly, both lbph and lcp seem to have a big group of data points with the same highly negative value. This value will be further discussed underneath.
Thirdly, there seems to be some outliers that might influence the predictions.</p>
<p>Overall, all variables have different ranges. If this is not taken into account, some variables might have a higher weight on the predictions. In this case, however, it should not pose a problem as the glm function in R automatically standardizes the data.</p>
<p>We make svi categorial and find that 76 data points do not have ‘svi’ and 21 do. The variable is now ready to be used for predictions
0 1
76 21</p>
<p>Looking at the variables lbph and lcp, we find that the most negative point is -1.36 with almost half the data having this x-value for both variables. We assume this means that lbph and lcp are some kind of marker that is not always detectable in the blood and hence are given this value in case the marker is not found. Given we do not have more information about the variables, we leave them as they are.</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">&gt;	min(prostate$lbph) [1] -1.386294
&gt;	min(prostate$lcp) [1] -1.386294
&gt;	sum(prostate$lbph &lt;= -1.3862) [1] 43
&gt;	sum(prostate$lcp &lt;= -1.3862) [1] 45
</code></pre></div><p>Finally, checking for the outliers seen on the graphs above, we looked for data points that were 3 times the interquartile range above the upper quartile or bellow the lower quartile.</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">[[1]]
[1] 373.0657 216.9818

[[2]]
numeric(0)

[[3]]
[1] 6.10758

[[4]]
numeric(0)

[[5]]
numeric(0)

[[6]]
[1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

[[7]]
numeric(0)

[[8]]
numeric(0)
</code></pre></div><p>We found that Cscore and lweight had outliers in this category while cvi (list 6) cannot have outliers as it is a categorial variable. We decided, however, to keep the outliers in the data set due to a lack of understanding of how these variables were measured and whether the outliers were a typo or real possible number.</p>
<p>Finally, to understand the variables, we performed a correlation plot to see how the variables are relating to Cscore and each other. It is important to check for possible collinearity as Lasso can specifically be used to counter this problem.</p>
<p><img src="/images/svi.jpg" alt="svi"></p>
<p>Looking at the correlation matrix, we see that svi, lcp, lpsa and lcavol are the four variables most highly correlated to Cscore. These variables will probably predict Cscore the best. However, we see that these same four variables are also rather highly correlated to each other meaning that there is some multicollinearity.</p>
<p>The presence of this phenomenon can have a negative impact on our analysis as a whole and specifically the variance. Therefore, we will have to use a shrinkage technique such as Lasso to select an appropriate subset of our variables.</p>
<p><strong>2. LASSO model</strong></p>
<p>In order to run a lasso model, we created a matrix x and vector y and split up our data in test and train data for a 80-20 split. Next, we ran a lasso model on our training data and plotted the coefficients of each variable against the ℓ1-norm of the whole coefficient vector as λ varies. We observe that the lasso first results a model that contains only lpsa. Then lcp and svi enter almost simultaneously. Next is lweight and lcavol before the remaining variables enter the model.</p>
<p><img src="/images/L1norm.png" alt="L1norm"></p>
<p>Next, we need to find the optimal lambda for the model. We use cross validation to find the optimal lambda. Underneath, you can see the value of lambda plotted against the MSE of the training data.</p>
<p><img src="/images/MSE.png" alt="MSE"></p>
<p>As seen on the graph above, the lambda for the smallest MSE (minlambda) on the training data returns 3 non-zero variables while the lambda within one SE (lambda1se) returns a model with one non-zero variable.
We compared the MSE on the test data using both lambdas and found that the minlambda gave the best test MSE. (see underneath)</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">&gt;	MSE.minlambda [1] 639.5722

&gt;	MSE.lambda1se [1] 793.1503
</code></pre></div><p>Next, we ran the lasso model on the whole dataset using the minlambda and found the coefficients of all variables.</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">&gt;	lasso.coef
(Intercept)	lcavol	lweight	age	lbph	svi	lcp lpsa
-26.046484	0.000000	0.000000	0.000000	0.000000	17.677799	3.628364 23.814752
</code></pre></div><p>Finally, we see that Lasso gives us a 3 non-zero variables model with coefficients 23.81, 17.68, and 3.63 of lpsa, svi, and lcp, respectively. The intercept of the model is -26.05 meaning that every person starts off with a negative score. This then increases the higher the lcp and lpse. Whenever a person has “1” for svi, its score will go up by 17.68.
Comparing the final Lasso model to variables that correlated highly with Cscore or not, the model does not seem too surprising; all variables that correlated highly with Cscore are still in the model, except for lcavol which was forced to zero by Lasso.
The reason for this will be discussed further in the next chapter.</p>
<p><strong>3. Does &ldquo;Lcacol&rdquo; correspond to how wll it can predict Cscore?</strong></p>
<p>When looking at the correlation matrix, the correlation between lcavol and Cscore was rather high meaning that the variable could explain Cscore well. In contrast, the coefficient of lcavol was reduced to zero in the Lasso model. Therefore, the coefficient does not correspond to how well the variable originally seemed to be able to predict Cscore.
This can be explained by the correlation between the lcavol and the other variables that are not zero in the Lasso model. Lasso is specifically used when the dataset has multiple variables that correlate. It penalizes and forces coefficients of variables to zero when there are too many that do not add much to the model and might cause a high variance.
As the other variables such as lpsa explained Cscore a bit better, Lasso chose to keep those and reduce lcavol’s coefficient to zero.</p>
<p><strong>4. Model with Non-linear effects</strong></p>
<p>Next, we wanted to fit and compare different models with different polynomials for the variab les that Lasso gave us. The problem was that there were many possible polynomials and comparing all of them manually would take a long time. We therefore found a function called “polywog” which not only compares different polynomials using cross validation but also includes interactions between the different variables. Additionally, the function deletes variables that are exactly the same (e.g. svi^2 is deleted because svi is categorial and 1^2 equals 1) before it uses Lasso to prevent overfitting. More about this function can be found h ere: <a href="https://cran.r-project.org/web/packages/polywog/polywog.pdf">https://cran.r-project.org/web/packages/polywog/polywog.pdf</a>.</p>
<p>We fitted the model on the same training data as used for Lasso with the three variables given by the previous model. For each degree of polynomial, the function returns the lambda involved and error rate. (see below)</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">&gt;	cv1$results
	degree lambda.min	cv.err
[1,]	1 7.9974302 1659.6030
[2,]	2 0.6287708 913.3407
[3,]	3 0.1690634 670.3135
[4,]	4 18.6840829 716.2160

</code></pre></div><p>We see that the third polynomial gives the lowest error rate. The polywog function automatically chooses the correct degree and can be used to predict the MSE of the test data.
The test MSE gives 277.28, which is significantly lower than Lasso’s test MSE of 639.57.</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">&gt;	MSE.polywog.3v [1] 277.2717
</code></pre></div><p>Although we previously said we would only use the variables given by the original Lasso model, we wanted to check if the model would generate better results when lcavol was added back.</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">&gt;	cv2$results
	degree lambda.min	cv.err
[1,]	1 0.07561102 1606.8289
[2,]	2 3.96883460 827.1445
[3,]	3 5.58414921 799.1701
[4,]	4 86.34432040 959.1398

&gt;	MSE.polywog.4v [1] 677.2405
</code></pre></div><p>As seen above, this was not the case. Both the error of the training data and MSE of the test data of the model with now 4 variables (polywog.4v) generated a higher error than the polywog model with 3 variables (polywog.3v).
We therefore conclude that the polywog model with 3 variables is the best model with appropriate non-linear effects with a MSE test of 277.28 compared to 677.24 of the polywog.4v and 639.57 of the Lasso model.
Finally, it would be interesting to look at the coefficients generated polywog.3v model.</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">&gt;	cv1$polywog.fit$coefficients
(Intercept)	svi	lcp	lpsa	svi.lcp
0.147715677	-144.665137258	0.000000000	4.775317200	179.383763409
svi.lpsa	lcp^2	lcp.lpsa	lpsa^2	svi.lcp^2
0.001381793	6.342430677	0.000000000	-4.010743051	-17.063534984
svi.lcp.lpsa	svi.lpsa^2	lcp^3	lcp^2.lpsa	lcp.lpsa^2
-46.019161095	11.851694891	0.661667904	-1.218838544	1.025148043
lpsa^3				
1.980807662				
</code></pre></div><p>As mentioned above, the function deleted svi^2 because svi would generate the exact same result. Multiple interaction variables between all three variables or a combination are added to the model too (e.g. lcp.lpsa, svi.lcp.lpsa) making for a very strong model.</p>
<p><strong>5. Conclusion</strong></p>
<p>This project tried to generate the best possible model to predict Cscore, which describes the progression of prostate cancer, based on multiple variables.
Overall, we created two main models polywog and Lasso model. The latter generated coefficients for three variables: svi, lcp, and lpsa. The polywog model generated coefficients for these three variables as well as for newly created variables which were interactions between these three variables as well as polynomials of them.
The models generated a test MSE of 639.57 and 277.28, respectively.</p>
<hr>

  </article>
</section>


      </div>
      
        <footer class="footer">
  <section class="container">
    
      <div class="sns-shares sp-sns-shares">
        
          <a class="sns-share twitter-share" href="https://twitter.com/intent/tweet?original_referer=%2fprojects%2f&ref_src=twsrc%5Etfw&text=Projects HOME&tw_p=tweetbutton&url=%2fprojects%2f"><i class="fab fa-twitter"></i></a>
        
        
          <a class="fb btn sns-share fb-share" href="http://www.facebook.com/share.php?u=%2fprojects%2f" onclick="window.open(this.href, 'FBwindow', 'width=650, height=450, menubar=no, toolbar=no, scrollbars=yes'); return false;"><i class="fab fa-facebook-f"></i></a>
        
        
        
        
          <a class="sns-share linkedIn-share" href="https://www.linkedin.com/sharing/share-offsite/?url=%2fprojects%2f"><i class="fab fa-linkedin"></i></a>
        
      </div>
    
    
     © 2018    ·  Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/naro143/hugo-coder-portfolio">CoderPortfolio</a>. 

  </section>
</footer>
<div class="fixed-bar">
  <section class="container">
    
      <p id="privateTriggerText">Do you want to contact me via e-mail?→<a id="privateTrigger">Click!</a></p>
    
    
      <div class="sns-shares pc-sns-shares">
        
          <a class="sns-share twitter-share" href="https://twitter.com/intent/tweet?original_referer=%2fprojects%2f&ref_src=twsrc%5Etfw&text=Projects HOME&tw_p=tweetbutton&url=%2fprojects%2f"><i class="fab fa-twitter"></i></a>
        
        
          <a class="fb btn sns-share fb-share" href="http://www.facebook.com/share.php?u=%2fprojects%2f" onclick="window.open(this.href, 'FBwindow', 'width=650, height=450, menubar=no, toolbar=no, scrollbars=yes'); return false;"><i class="fab fa-facebook-f"></i></a>
        
        
        
        
          <a class="sns-share linkedIn-share" href="https://www.linkedin.com/sharing/share-offsite/?url=%2fprojects%2f"><i class="fab fa-linkedin"></i></a>
        
      </div>
    
  </section>
</div>

      
    </main>

    

  <script src="/js/app.js"></script>
  
  <script>
  (function($) {
    $(function() {
      $('#privateTrigger').on('click', function() {
        $('.private').slideToggle();
        $('#privateTriggerText').text("manuv.mfa2021@london.edu | Please share my profile if you like it→");
      });
    });
   })(jQuery);
  </script>
  
  </body>
</html>
